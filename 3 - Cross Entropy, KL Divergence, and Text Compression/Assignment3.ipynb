{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495997a0",
   "metadata": {},
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: Rayyan Mohammad Minhaj\n",
    "\n",
    "Student id 1: 7074982\n",
    "\n",
    "Email 1:rami00002@stud.uni-saarland.de\n",
    "\n",
    "Name 2: Abdullah Abdul Wahid\n",
    "\n",
    "Student id 2: 7075730\n",
    "\n",
    "Email 2:  abyy00002@stud.uni-saarland.de\n",
    "\n",
    "Instructions: Read each question carefully. \n",
    "\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. \n",
    "\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of Name1_studentID1_Name2_studentID2.zip. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after your upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3739a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp39-cp39-win_amd64.whl (10.2 MB)\n",
      "     -------------------------------------- 10.2/10.2 MB 129.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\rayya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas==1.3.5) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\rayya\\appdata\\roaming\\python\\python39\\site-packages (from pandas==1.3.5) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\rayya\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas==1.3.5) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rayya\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.17.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "Successfully installed pandas-1.3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\rayya\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas==1.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dae4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from math import log2\n",
    "from typing import List, Dict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2676c",
   "metadata": {},
   "source": [
    "## Exercise 1: Cross-Entropy and KL-Divergence (6 points)\n",
    "\n",
    "### Theory\n",
    "\n",
    "Recall the formulas for Cross-Entropy:\n",
    "\n",
    "$$H(P, Q) = -\\sum_{x \\in X} P(x) \\times \\log{Q(x)}$$\n",
    "\n",
    "And KL-Divergence:\n",
    "\n",
    "$$D_{KL}(P || Q) = \\sum_{x \\in X} P(x) \\times \\log{\\frac{P(x)}{Q(x)}}$$\n",
    "\n",
    "where P often signifies the empirical or observed probability distribution and Q signifies the estimated distribution.\n",
    "\n",
    "\n",
    "1. A common way to train a language model is to minimize the Cross-Entropy score. Explain why minimizing Cross-Entropy is equivalent to minimizing KL-Divergence. Support your answer with a mathematical expression. [1 point] <br/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
    "\n",
    "    $\\forall x,y,z \\in U:$\n",
    "\n",
    "    1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
    "    2. $d(x,y) = d(y,x)$\n",
    "    3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
    "\n",
    "    Is $D_{KL}$ a distance metric? ($U$ in this case is the set of all distributions over the same possible states).\n",
    "For each of the three points either prove that it holds for KDL​ or show a counterexample why not. [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7b233",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "1 - While training a model, we want Q (the predictions) to get as close as possible to P (ground truth). We also know that Entropy (H) measures the uncertainty of truth, cross-entropy measures the avg bits needed to encode samples from P to Q, and KL-Divergence measures the how much difference there is between model and actual truth. So we can mathematically say: <br/>\n",
    "\n",
    "$H(P,Q) = H(P) + D_{KL}(P||Q)$ <br/>\n",
    "\n",
    "Thus, minimizing Cross-Entropy is equivalent to minimizing KL-Divergence because the only part that depends on the model’s predictions is the KL part. The other part (entropy of the real data - H(P)) is fixed.\n",
    "\n",
    "<br/>\n",
    "\n",
    "2- For the first property, yes it holds and we can check this with a simple calculation:<br/>\n",
    "$x=y=3$ <br/>\n",
    "$D_{KL}(x||y) = 3 * log(3/3) = 0$ <br/>\n",
    "This is mainly because log(1) is equal to 0. <br/>\n",
    "\n",
    "<br/>\n",
    "For second property, it does not hold as KL-Divergence is not symmetric. We can verify this using a simple counter example: <br/>\n",
    "\n",
    "$x=P=3$ and $y=Q=4$ <br/>\n",
    "$D_{KL}(x||y) = 3 * log(3/4) = -0.86$ <br/>\n",
    "$D_{KL}(y||x) = 4 * log(4/3) = 1.15$ <br/>\n",
    "\n",
    "<br/>\n",
    "For last property, it also does not hold, again we can verify this using a simple counter example: <br/>\n",
    "\n",
    "$x=3$ and $y=4$ and $z=5$<br/>\n",
    "$D_{KL}(x||z) = 3 * log(3/5) = -1.53$ <br/>\n",
    "$D_{KL}(x||y) = 3 * log(3/4) = -0.86$ <br/>\n",
    "$D_{KL}(y||z) = 4 * log(4/5) = -0.89$ <br/>\n",
    "\n",
    "$-1.53 <= -1.75$ <br/>\n",
    "therefore, we can say KL-Divergence is not a distance measure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f502fb9",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Suppose we have three coins. Here are the results of flipping each coin 20 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f33f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin1 = \"H T H T H T H T H T H T H T H T H T H T\"\n",
    "coin2 = \"H H H H T H H T H H H T T H H T H H H H\"\n",
    "coin3 = \"T T T T T T T T T T T T T H T T T T H T\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e2aeb",
   "metadata": {},
   "source": [
    "Let's turn the sequences into lists of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489b44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s : str) -> List[str]:\n",
    "    return s.split()\n",
    "\n",
    "coin1_tokens = tokenize(coin1)\n",
    "coin2_tokens = tokenize(coin2)\n",
    "coin3_tokens = tokenize(coin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c759beb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'T', 'H', 'T', 'H', 'T', 'H', 'T', 'H', 'T', 'H', 'T', 'H', 'T', 'H', 'T', 'H', 'T', 'H', 'T']\n"
     ]
    }
   ],
   "source": [
    "print(coin1_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335b2d6",
   "metadata": {},
   "source": [
    "Write the methods for a unigram model that\n",
    "1. Estimate a probability distribution, given the tokenized text (use the imported `Counter`). Make sure that it is possible to update the model's distribution. [0.5 points]\n",
    "2. Calculate the cross entropy between the model's estimated distribution and some given probability distribution. [1 point]\n",
    "3. Calculate the KL-Divergence. [1 point]\n",
    "\n",
    "**NOTE**: So far, we haven't covered strategies for dealing with out-of-vocabulary tokens. For now, we will accept if you:\n",
    " * Include only the tokens that are present in both distributions when calculating Cross-Entropy and KL-Divergence, i.e. ignore the tokens that don't appear in both distributions.\n",
    " * Normalize the resulting distributions so that they sum up to one.\n",
    "\n",
    "Feel free to write separate methods for those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c24b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dist = {}\n",
    "        self.freq = {}\n",
    "    \n",
    "    def fit(self, data: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Define a probability distribution for the model\n",
    "        and assign it to self.dist\n",
    "        \n",
    "        Args:\n",
    "            data - list of tokens\n",
    "        \"\"\"\n",
    "        self.freq = Counter(data) #this is just simple freq of heads and tails\n",
    "\n",
    "        count = Counter(data)\n",
    "        total = sum(count.values())\n",
    "\n",
    "        for token, count in count.items():\n",
    "            self.dist[token] = count/total \n",
    "\n",
    "    def cross_entropy(self, distribution: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate the Cross-Entropy between the model's and a given distribution\n",
    "        \n",
    "        Args:\n",
    "            distribution - dictionary of token probabilities\n",
    "        Returns:\n",
    "            cross_entropy - the Cross-Entropy value\n",
    "        \"\"\"\n",
    "        # my P and Q look like: {'H':0.5, 'T':0.5}\n",
    "        #H(P,Q) = - SUM -> P(x) * log(Q(x)) ------ here P(x) is basically head, tails self.dist and Q(x) is the distribution\n",
    "\n",
    "        cross_entropy = 0\n",
    "\n",
    "        for i in range(0, len(self.dist.items())):\n",
    "            #print(f'P({i}): {list(self.dist.values())[i]} and Q({i}): {list(distribution.values())[i]}')\n",
    "            cross_entropy +=list(self.dist.values())[i] * log2(list(distribution.values())[i])\n",
    "        \n",
    "        cross_entropy = -(cross_entropy)\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "    \n",
    "    def kl_divergence(self, distribution: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate the KL divergence between the model's and a given distribution\n",
    "\n",
    "        Args:\n",
    "            distribution - dictionary of token probabilities\n",
    "        Returns:\n",
    "            kl_divergence - the KL-Divergence value\n",
    "        \"\"\"\n",
    "        #KLD (P||Q) = SUM-> P(x) * log(P(x)/Q(x))\n",
    "\n",
    "        kl_divergence = 0\n",
    "\n",
    "        for i in range(0, len(self.dist.items())):\n",
    "            #print(f'P({i}): {list(self.dist.values())[i]} and Q({i}): {list(distribution.values())[i]}')\n",
    "            kl_divergence +=list(self.dist.values())[i] * log2(list(self.dist.values())[i]/list(distribution.values())[i])\n",
    "        \n",
    "\n",
    "        return kl_divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f16e6",
   "metadata": {},
   "source": [
    "Now fit the models on the provided coins and print out the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04472ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 estimated probabilities\n",
      "H: 0.5000\n",
      "T: 0.5000\n",
      "\n",
      "Model 2 estimated probabilities\n",
      "H: 0.7500\n",
      "T: 0.2500\n",
      "\n",
      "Model 3 estimated probabilities\n",
      "T: 0.9000\n",
      "H: 0.1000\n"
     ]
    }
   ],
   "source": [
    "coin_model1 = UnigramModel()\n",
    "coin_model2 = UnigramModel()\n",
    "coin_model3 = UnigramModel()\n",
    "\n",
    "coin_model1.fit(coin1_tokens)\n",
    "coin_model2.fit(coin2_tokens)\n",
    "coin_model3.fit(coin3_tokens)\n",
    "\n",
    "print(\"Model 1 estimated probabilities\")\n",
    "for token, prob in sorted(coin_model1.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Model 2 estimated probabilities\")\n",
    "for token, prob in sorted(coin_model2.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Model 3 estimated probabilities\")\n",
    "for token, prob in sorted(coin_model3.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9d1f6",
   "metadata": {},
   "source": [
    "Update Model 2 with some additional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73dcafa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 updated estimated probabilities\n",
      "H: 0.7879\n",
      "T: 0.2121\n"
     ]
    }
   ],
   "source": [
    "coin2_ext = \"H H T H H H T T H H H H H H H H H T T H H H H H H T H H H T H H H\"\n",
    "coin2_ext_tokens = tokenize(coin2_ext)\n",
    "coin_model2.fit(coin2_ext_tokens)\n",
    "\n",
    "print(\"Model 2 updated estimated probabilities\")\n",
    "for token, prob in sorted(coin_model2.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696a26c",
   "metadata": {},
   "source": [
    "Let's assume the empirical probability distribution for all coins is actually uniform. Calculate the Cross-Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d73d23fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy with Uniform Distribution\n",
      "Model 1 Cross-Entropy: 1.0\n",
      "Model 2 Cross-Entropy: 1.0\n",
      "Model 3 Cross-Entropy: 1.0\n"
     ]
    }
   ],
   "source": [
    "uniform_coin_dist = {\n",
    "    \"H\": 0.5,\n",
    "    \"T\": 0.5\n",
    "}\n",
    "\n",
    "print(\"Cross-Entropy with Uniform Distribution\")\n",
    "\n",
    "print(\"Model 1 Cross-Entropy:\", coin_model1.cross_entropy(uniform_coin_dist))\n",
    "print(\"Model 2 Cross-Entropy:\", coin_model2.cross_entropy(uniform_coin_dist))\n",
    "print(\"Model 3 Cross-Entropy:\", coin_model3.cross_entropy(uniform_coin_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a86569",
   "metadata": {},
   "source": [
    "Try it out with another distribution of your choosing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e93256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy with Distribution:\n",
      "Model 1 Cross-Entropy: 1.736965594166206\n",
      "Model 2 Cross-Entropy: 2.649519761248084\n",
      "Model 3 Cross-Entropy: 3.004935594743131\n"
     ]
    }
   ],
   "source": [
    "coin_dist = {\n",
    "    \"H\": 0.1,\n",
    "    \"T\": 0.9\n",
    "}\n",
    "\n",
    "print(\"Cross-Entropy with Distribution:\")\n",
    "print(\"Model 1 Cross-Entropy:\", coin_model1.cross_entropy(coin_dist))\n",
    "print(\"Model 2 Cross-Entropy:\", coin_model2.cross_entropy(coin_dist))\n",
    "print(\"Model 3 Cross-Entropy:\", coin_model3.cross_entropy(coin_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7b952",
   "metadata": {},
   "source": [
    "Calculate KL-Divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61aedd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL-Divergence with Uniform Distribution:\n",
      "Model 1 KL Divergence: 0.0\n",
      "Model 2 KL Divergence: 0.25448215718917155\n",
      "Model 3 KL Divergence: 0.5310044064107189\n"
     ]
    }
   ],
   "source": [
    "print(\"KL-Divergence with Uniform Distribution:\")\n",
    "print(\"Model 1 KL Divergence:\", coin_model1.kl_divergence(uniform_coin_dist))\n",
    "print(\"Model 2 KL Divergence:\", coin_model2.kl_divergence(uniform_coin_dist))\n",
    "print(\"Model 3 KL Divergence:\", coin_model3.kl_divergence(uniform_coin_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48356c11",
   "metadata": {},
   "source": [
    "In the `data` folder you are provided with:\n",
    "\n",
    "* `unigram_freq.csv`: a file containing information on ~300k top words and their counts, derived from the Google Web Trillion Word Corpus (comma-separated).\n",
    "* `frankenstein.txt`: the novel \"Frankenstein\" by Mary Shelly.\n",
    "* `wikipedia.txt`: English Wikipedia corpus.\n",
    "* `code.txt`: A small corpus of code, taken from the [codeparrot/github-code](https://huggingface.co/datasets/codeparrot/github-code) dataset.\n",
    "\n",
    "To load and tokenize the texts, feel free to reuse the functions you wrote in your first assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b4f5cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filepath: str = 'data.txt') -> str:\n",
    "    \"\"\"\n",
    "    Load text from a file.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the file to be loaded.\n",
    "    Returns:\n",
    "        The content of the file as a string.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            contents= f.read()\n",
    "    \n",
    "    return contents\n",
    "\n",
    "def preprocess(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess the input text by lowercasing and removing punctuation.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens after preprocessing.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    cleaned_text = ''.join(char for char in text if char not in string.punctuation)\n",
    "    tokens = cleaned_text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54e498",
   "metadata": {},
   "source": [
    "Load the `unigram_freq.csv` data (use `pandas`) and create a dictionary representing a probability distribution (word: probability). Put the dictionary into a variable `empirical_dist`. [0.5 points]\n",
    "\n",
    "For the sake of this exercise, we will assume it as the true probability distribution of American English and that no other words appear in the distribution apart from those listed in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "77f2bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  word        count\n",
      "0  the  23135851162\n",
      "1   of  13151942776\n",
      "2  and  12997637966\n",
      "3   to  12136980858\n",
      "4    a   9081174698\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/unigram_freq.csv')\n",
    "print(df.head())\n",
    "\n",
    "total = 0\n",
    "samp_dict = {}\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    total += df.iloc[i,1]\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    samp_dict[df.iloc[i,0]] = df.iloc[i,1]/total\n",
    "\n",
    "empirical_dist =  samp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7938746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588124220187\n"
     ]
    }
   ],
   "source": [
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "378891a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 0.03933837507090547\n",
      "of: 0.022362525338300483\n",
      "and: 0.022100157619537028\n",
      "to: 0.020636764209678228\n",
      "a: 0.015440912627459126\n",
      "in: 0.014400707674149294\n",
      "for: 0.010088551882990708\n",
      "is: 0.008001275333472512\n",
      "on: 0.006376923565241907\n",
      "that: 0.005781144503654221\n",
      "by: 0.005696158661744654\n",
      "this: 0.005489435157037871\n",
      "with: 0.0054123101306521575\n",
      "i: 0.0052475738476111455\n",
      "you: 0.005094469709217781\n",
      "it: 0.004783281792247097\n",
      "not: 0.00447777365836533\n",
      "or: 0.004405089635955221\n",
      "be: 0.004078601220057391\n",
      "are: 0.00406991378324621\n"
     ]
    }
   ],
   "source": [
    "#printing the first 20 here because printing the entire dict was taking too long :(\n",
    "from itertools import islice\n",
    "\n",
    "for key, value in islice(empirical_dist.items(), 20):\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c02864",
   "metadata": {},
   "source": [
    "Calculate the following Cross-Entropy and KL-Divergence Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4f6069b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frankenstein Cross-Entropy with Empirical Distribution: 11.424272427328797\n",
      "Frankenstein KL Divergence with Empirical Distribution: 1.9832825634349243\n"
     ]
    }
   ],
   "source": [
    "text_frankenstein = load_text('data/frankenstein.txt')\n",
    "tokens_frankenstein = preprocess(text_frankenstein)\n",
    "\n",
    "model_frankenstein = UnigramModel()\n",
    "model_frankenstein.fit(tokens_frankenstein)\n",
    "\n",
    "print(\"Frankenstein Cross-Entropy with Empirical Distribution:\", model_frankenstein.cross_entropy(empirical_dist))\n",
    "print(\"Frankenstein KL Divergence with Empirical Distribution:\", model_frankenstein.kl_divergence(empirical_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "45f9410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Cross-Entropy with Empirical Distribution: 13.55586777034593\n",
      "Wikipedia KL Divergence with Empirical Distribution: 1.9832013262654153\n"
     ]
    }
   ],
   "source": [
    "text_wikipedia = load_text('data/wikipedia.txt')\n",
    "tokens_wikipedia = preprocess(text_wikipedia)\n",
    "\n",
    "model_wikipedia = UnigramModel()\n",
    "model_wikipedia.fit(tokens_wikipedia)\n",
    "print(\"Wikipedia Cross-Entropy with Empirical Distribution:\", model_wikipedia.cross_entropy(empirical_dist))\n",
    "print(\"Wikipedia KL Divergence with Empirical Distribution:\", model_wikipedia.kl_divergence(empirical_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9cf8bc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Cross-Entropy with Empirical Distribution: 15.669226023405667\n",
      "Code KL Divergence with Empirical Distribution: 2.9864689518795524\n"
     ]
    }
   ],
   "source": [
    "text_code = load_text('data/code.txt')\n",
    "tokens_code = preprocess(text_code)\n",
    "\n",
    "model_code = UnigramModel()\n",
    "model_code.fit(tokens_code)\n",
    "print(\"Code Cross-Entropy with Empirical Distribution:\", model_code.cross_entropy(empirical_dist))\n",
    "print(\"Code KL Divergence with Empirical Distribution:\", model_code.kl_divergence(empirical_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e6873",
   "metadata": {},
   "source": [
    "## Text Compression (4 points)\n",
    "\n",
    "Let's say we want to compress our datasets with a prefix-free binary code.\n",
    "\n",
    "1. Write a function that computes the optimal length for each word in a given distribution. [0.5 points]\n",
    "<!-- As explained in the lecture, a nice way of constructing a code would be, is to determine the length of the encoding a token based on the frequency of the token. This can be done in many ways. In the lecture we talked about prefix codes:\n",
    "No code word is a prefix of another code wordWe can organize the code as a tree\n",
    "\n",
    "Given an arbitrary alphabet along with probabilities for each token, you are to implement a function that outputs the encoding for each character. (3 points.)\n",
    "\n",
    "**HINT**: feel free to use the example in the slides to validate that your generated encoding is correct:\n",
    "\n",
    "\n",
    "| word | frequency | $C(\\text{word})$ |\n",
    "|:-----|:----------|:-----------------|\n",
    "| the  |0.5        |`0`               |\n",
    "| and  |0.25       |`10`              |\n",
    "| of   |0.125      |`110`             |\n",
    "| he   |0.125      |`111`             |\n",
    "\n",
    "\n",
    "Where $C(\\text{word})$ represents the encoding of word.\n",
    "\n",
    "Though this algorithm is generalizable to any base of the code (i.e. the code need not be binary), we shall limit this exercise to binary encoding. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORMULA IS IN SLIDE 35 of LECTURE 4\n",
    "from math import ceil\n",
    "\n",
    "def optimal_binary_length(distribution : Dict[str, float]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Calculate the optimal binary length for a given distribution.\n",
    "\n",
    "    Args:\n",
    "        distribution: A dictionary of token probabilities.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with tokens as keys and their optimal binary lengths as values.\n",
    "    \"\"\"\n",
    "    # optimal len = ciel(-log2(p)) --- log2 here because theyre asking for binary len\n",
    "    optimal_dict = {}\n",
    "    for key, value in distribution.items():\n",
    "        optimal_dict[key] = ceil(-log2(value))\n",
    "    \n",
    "    return optimal_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0c129ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Encoding Length: \"if\"\n",
      "Frankenstein:\t9.0000\n",
      "Wikipedia:\t12.0000\n",
      "Code:\t\t7.0000\n",
      "\n",
      "Optimal Encoding Length: \"the\"\n",
      "Frankenstein:\t5.0000\n",
      "Wikipedia:\t4.0000\n",
      "Code:\t\t6.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Encoding Length: \\\"if\\\"\")\n",
    "print(f\"Frankenstein:\\t{optimal_binary_length(model_frankenstein.dist)['if']:.4f}\")\n",
    "print(f\"Wikipedia:\\t{optimal_binary_length(model_wikipedia.dist)['if']:.4f}\")\n",
    "print(f\"Code:\\t\\t{optimal_binary_length(model_code.dist)['if']:.4f}\")\n",
    "print()\n",
    "print(\"Optimal Encoding Length: \\\"the\\\"\")\n",
    "print(f\"Frankenstein:\\t{optimal_binary_length(model_frankenstein.dist)['the']:.4f}\")\n",
    "print(f\"Wikipedia:\\t{optimal_binary_length(model_wikipedia.dist)['the']:.4f}\")\n",
    "print(f\"Code:\\t\\t{optimal_binary_length(model_code.dist)['the']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9cace",
   "metadata": {},
   "source": [
    "2. Write a function that returns an expected code length of a token sequence, given a probability distribution. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "beca2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGAIN, FORMULA IS IN SLIDE 35 of LECTURE 4\n",
    "\n",
    "def expected_code_length(tokens: List[str], distribution: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the expected code length for a given distribution.\n",
    "\n",
    "    Args:\n",
    "        distribution: A dictionary of token probabilities.\n",
    "    Returns:\n",
    "        float: The expected code length.\n",
    "    \"\"\"\n",
    "    #expected code len = SUM-> (optimal bin len)* p(wi)\n",
    "    #again optimal bin len is = ciel(-log2(p))\n",
    "\n",
    "    expected_code_len = 0\n",
    "    for token in tokens:\n",
    "        p = distribution.get(token, 0)\n",
    "        expected_code_len += p * ceil(-log2(p))\n",
    "    \n",
    "    return expected_code_len\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ddae0878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Code Length for Frankenstein: 4729.49492918808\n",
      "Number of tokens in Frankenstein: 78094\n",
      "\n",
      "Expected Code Length for Wikipedia: 78275.76804626374\n",
      "Number of tokens in Wikipedia: 1800837\n",
      "\n",
      "Expected Code Length for Code: 6446.640550953427\n",
      "Number of tokens in Code: 448389\n"
     ]
    }
   ],
   "source": [
    "expected_length_frankenstein = expected_code_length(tokens_frankenstein, model_frankenstein.dist)\n",
    "expected_length_wikipedia = expected_code_length(tokens_wikipedia, model_wikipedia.dist)\n",
    "expected_length_code = expected_code_length(tokens_code, model_code.dist)\n",
    "\n",
    "print(\"Expected Code Length for Frankenstein:\", expected_length_frankenstein)\n",
    "print(\"Number of tokens in Frankenstein:\", len(tokens_frankenstein))\n",
    "print()\n",
    "print(\"Expected Code Length for Wikipedia:\", expected_length_wikipedia)\n",
    "print(\"Number of tokens in Wikipedia:\", len(tokens_wikipedia))\n",
    "print()\n",
    "print(\"Expected Code Length for Code:\", expected_length_code)\n",
    "print(\"Number of tokens in Code:\", len(tokens_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1f872",
   "metadata": {},
   "source": [
    "3. Consider four binary codes.\n",
    "\n",
    "| word | $C_1(\\text{word})$| $C_2(\\text{word})$| $C_3(\\text{word})$| $C_4(\\text{word})$|\n",
    "|:-----|:------------------|:------------------|:------------------|:------------------|\n",
    "| the  |`100`              |`0`                |`0`                |`11`               |\n",
    "| and  |`01`               |`10`               |`10`               |`110`              |\n",
    "| of   |`110`              |`110`              |`110`              |`1011`             |\n",
    "| and  |`1110`             |`1110`             |`111`              |`0`                |\n",
    "| to   |`1111`             |`1111`             |`1111`             |`1101`             |\n",
    "\n",
    "* Which of these codes are prefix-free? For other codes, explain why they are not. [0.5 points]\n",
    "\n",
    "* Which of these codes satisfy Kraft's inequality? [0.5 points]\n",
    "\n",
    "4. Prove mathematically that Kraft's inequality holds for all prefix-free binary codes. **HINT**: think about how many leaves there are at a binary tree's depth $l_n$. [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf0143",
   "metadata": {},
   "source": [
    "## Q3 Answers\n",
    "We can tell if a code is prefix-free if it does occur in the beginning of any other code, for ex, Ci and Cj are prefixes of each other if Ci's code starts with C_j's or vice versa. By this logic we can tell: \n",
    "- for C1, it is prefix free because no code occurs in the beginning of any other\n",
    "- for C2, it is prefix free because no code occurs in the beginning of any other\n",
    "- for C3, it is NOT prefix free because the code for \"and\" occurs in the beginning of \"to\"\n",
    "- for C4, it is NOT prefix free because the code foe \"the\" occurs in the beginning of \"and\"\n",
    "\n",
    "<br/>\n",
    "to check if they satisfy Kraft's inequality we need to check for \n",
    "\n",
    "$$\\sum_{i} D^{-l_i} <= 1$$\n",
    "\n",
    "where D is 2 (since binary) <br/>\n",
    "\n",
    "For C1:\n",
    "- 1/8 + 1/4 + 1/8 + 1/16 + 1/16 = 0.625 < 1 therefore it does satisfy the inequality \n",
    "\n",
    "For C2:\n",
    "- 1/2 + 1/4 + 1/8 + 1/16 + 1/16 = 1<=1 therefore it does satisfy the inequality \n",
    "\n",
    "For C3:\n",
    "- 1/2 + 1/4 + 1/8 + 1/8 + 1/16 = 1.06 <=1 therefore it does NOT satisfy the inequality \n",
    "\n",
    "For C4:\n",
    "- 1/4 + 1/8 + 1/16 + 1/2 + 1/16 = 1<=1 therefore it does satisfy the inequality \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbbb642",
   "metadata": {},
   "source": [
    "## Q4 Answers\n",
    "\n",
    "Imagine a full binary tree where each node has 2 children. At depth $l$ there are at most $2^l$ nodes. So at depth of $l_{i}$ there are at most  $2^l_{i}$ leaf positions. <br/>\n",
    "\n",
    "Once a codeword is placed at a leaf, none of its descendants can be used for any other codeword (to preserve prefix-freeness). <br/>\n",
    "Hence, placing a codeword at depth $l_i$ blocks off all $2^{L-l_i}$ leaves at the deeper level $L$ <br/>\n",
    "\n",
    "Now we can choose an $L$ that $L>= max(l_i)$. Imagine  extending all codewords to depth $L$. Each codeword of length $l_i$ has $2^{L-l_i}$ descendants at depth L. We can say these are the leaves it claims. <br/>\n",
    "\n",
    "Therefore, the sum of claimed leaves is $\\sum_{i=1} 2^{L-l_i}$ <br/>\n",
    "Since all code words must occupy non-overlapping set of leaves, the total number of claimed leaves cannot exceed the total number of leaves at depth L or rather $2^L$ <br/>\n",
    "So, $\\sum_{i=1} 2^{L-l_i} <= 2^L$ <br/>\n",
    "\n",
    "Divide both sides by $2^L$ and we get: <br/>\n",
    "\n",
    "$\\sum_{i=1} 2^{-l_i} <= 1$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
