{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d767ef",
   "metadata": {},
   "source": [
    "# SNLP Assignment 5\n",
    "\n",
    "Name 1: Rayyan Mohammad Minhaj<br/>\n",
    "Student id 1: 7074982<br/>\n",
    "Email 1: rami00002@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Abdullah Abdul Wahid <br/>\n",
    "Student id 2: 7075730 <br/>\n",
    "Email 2: abyy00002@stud.uni-saarland.de<br/> \n",
    "\n",
    "Name 3: <br/>\n",
    "Student id 3: <br/>\n",
    "Email 3: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2b723",
   "metadata": {},
   "source": [
    "## Exercise 1 - Back Off! Probabilities Again! (3 points)\n",
    "\n",
    "Consider the toy corpus:\n",
    "\n",
    ">    I study SNLP\\\n",
    ">    study SNLP I\\\n",
    ">    I like to study SNLP\\\n",
    ">    I study SNLP\n",
    "\n",
    "a. State the Vocabulary of the corpus and unigram counts. What is the bigram probability for the sequence \"I like SNLP\" using add-epsilon-discounting with ε = 0.5? (0.75 points)\n",
    "\n",
    "b. Now use a back-off bigram model with absolute discounting (use d=0.75) and a back-off to smoothed unigrams (with the same epsilon ε = 0.5) (0.75 points)\n",
    "\n",
    "c. Compare the result from both techniques. (0.5 points)\n",
    "\n",
    "d. To actually estimate these n-gram probabilities over a text corpus, we use **Maximum Likelihood Estimation (MLE)**. The estimate for the parameters of the MLE is obtained by getting counts from the corpus and then normalising them so they lie between 0 and 1. Consider the formula from the slides. \n",
    "\n",
    "$$P(w_2 | w_1) = \\frac{P(w_1,w_2)}{P(w_1)}$$\n",
    "\n",
    "Using this, state the empirical formula for finding the conditional probability of unigrams $P(w)$, bigrams $P(w_2|w_1)$, and trigrams $P(w_3|w_1,w_2)$ for a corpus of N words. We do not expect any mathematical proof here, but just the formula for finding the conditional probabilities from the words in the corpus using the shown equation as the starting point. (0.5 points)\n",
    "\n",
    "e. Using MLE, compute the probabilities for the following (assume we lowercase the corpus):  (0.5 points)\\\n",
    "(i) $p(snlp|study)$ \\\n",
    "(ii) $p(snlp|i \\ study)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81f333",
   "metadata": {},
   "source": [
    "# Answers - Exercise 1\n",
    "(a) The Vocabulary \"V\" (all unique words) is: V={\"I\", \"study\", \"SNLP\", \"like\", \"to\"} <br/>\n",
    "The Unigram counts for each word is: <br/>\n",
    "* \"I\" - 4\n",
    "* \"study\" - 4\n",
    "* \"SNLP\" - 4\n",
    "* \"like\" - 1\n",
    "* \"to\" - 1\n",
    "\n",
    "Then we need to get ALL possible bigrams:<br>\n",
    "* I study SNLP → Bigrams: (I, study), (study, SNLP)\n",
    "* study SNLP I → Bigrams: (study, SNLP), (SNLP, I)\n",
    "* I like to study SNLP → Bigrams: (I, like), (like, to), (to, study), (study, SNLP)\n",
    "* I study SNLP → Bigrams: (I, study), (study, SNLP)\n",
    "\n",
    "\n",
    "We want to compute bigram P(\"I like SNLP\") <br/>\n",
    "We can break down P( \"I like SNLP\" ) = P( \"like\" | \"I\" ) x P( \"SNLP\" | \"like\" )<br/>\n",
    "We use the formula:\n",
    "$$P(w_i | w_{i-1}) = \\frac{C(w_{i-1},w_i)+epsilon}{P(w_{i-1}) + epsilon*vocabulary size}$$\n",
    "the numerator is the count of times (I, like) occurs, and denominator is count of how many times (\"I\", \"WHATEVER\") occurs\n",
    "For $$P( \"like\" | \"I\" ) = \\frac{1+0.5}{3+(0.5)(5)} = 0.273$$\n",
    "For $$P( \"SNLP\" | \"like\" ) = \\frac{0+0.5}{1+(0.5)(5)} = 0.143$$\n",
    "Therefore, \n",
    "$$P( \"I like SNLP\" ) = P( \"like\" | \"I\" ) x P( \"SNLP\" | \"like\" ) = 0.273 * 0.143 = 0.039$$\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "(b) For abs discounting we use these formulas:\n",
    "$$ P(w_2|w1) = \\frac{C(w_1,w_2)-d}{C(w_1)} + \\alpha(w_1)*P_{unigram}(w_2) $$\n",
    "And if the bigram is unseen (its count is 0), then:\n",
    "$$ P(w_2|w_1) = \\alpha(w_1)*P_{unigram}(w_2) $$\n",
    "Now in a similar fashion, we will calculate P( \"like\" | \"I\" ) and P( \"SNLP\" | \"like\" )\n",
    "For $$P( \"like\" | \"I\" ) = \\frac{1-0.75}{3} + \\alpha(\"I\")*P_{unigram}(\"like\")$$\n",
    "But wait, we dont have $\\alpha$ yet. We can calculate it using the following formula:\n",
    "$$\\alpha(w)=\\frac{d.R(w)}{C(w)} = \\alpha(\"I\")=\\frac{0.75*2}{3} = 0.5$$\n",
    "SIDE NOTE: R(w) is basically the *distinct* counts thats why its 2, and the denominator is count of how many bigrams start with \"I\". <br/>\n",
    "We also need smoothed $P_{unigram}(\"like\")$\n",
    "$$P_{unigram}(\"like\") = \\frac{1+0.5}{14+0.5*5}=0.091$$\n",
    "\n",
    "Now back to\n",
    "$$P( \"like\" | \"I\" ) = \\frac{1-0.75}{3} + 0.5*0.091=0.129$$\n",
    "\n",
    "And for P( \"SNLP\" | \"like\" ), since it dosent exist, we use:\n",
    "$$P( \"SNLP\" | \"like\" ) = \\alpha(\"like\")*P_{unigram}(\"SNLP\")$$\n",
    "Im calculating the values behind the scenes,\n",
    "$$P( \"SNLP\" | \"like\" ) = 0.75*0.273 = 0.204$$\n",
    "\n",
    "And so finally,\n",
    "$$P(\"IlikeSNLP\")=P(like∣I)⋅P(SNLP∣like) = 0.129 x 0.204 = 0.0263$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "(c) The epsilon smoothing gives a higher probability because it over-smooths, giving even unseen bigrams events a non-negligible probability. Backing off is more accurate because it discounts seen bigrams realistically,and backs off to a more reasonable smoothed unigram distribution (rather than distributing probability mass evenly across ALL bigrams).\n",
    "<br/>\n",
    "\n",
    "(d) Unigram Probability:\n",
    "$$P(w) = \\frac{C(w)}{N}$$\n",
    "\n",
    "Bigram Probability:\n",
    "$$P(w_2|w_1) = \\frac{C(w_1,w_2)}{C(w_1)}$$\n",
    "\n",
    "Trigram Probability:\n",
    "$$P(w_3|w_1, w_2) = \\frac{C(w_1,w_2,w_3)}{C(w_1,w_2)}$$\n",
    "\n",
    "(e)(i) P(\"snlp\" | \"study\")\n",
    "$$P(\"snlp\" | \"study\") = \\frac{C(\"study \\ snlp\")}{C(\"study\")} = \\frac{4}{4} = 1$$\n",
    "\n",
    "(e)(ii) P(\"snlp\" | \"i\", \"study\")\n",
    "$$P(\"snlp\" | \"i\", \"study\") = \\frac{C(\"i \\ study \\ snlp\")}{C(\"i study\")} = \\frac{2}{2} = 1$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6f2da",
   "metadata": {},
   "source": [
    "## Exercise 2 - Good Turing Smoothing (2 points)\n",
    "\n",
    "Consider the following 2 tables\n",
    "\n",
    "<table style=\"display: inline-block; margin-right: 50px;\">\n",
    "  <caption><strong>Good-Turing Count-of-Counts</strong></caption>\n",
    "  <thead>\n",
    "    <tr><th>Count (N(w,h))</th><th>Count of Counts (n_N(w,h))</th></tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>1</td><td>5000</td></tr>\n",
    "    <tr><td>2</td><td>1600</td></tr>\n",
    "    <tr><td>3</td><td>1000</td></tr>\n",
    "    <tr><td>4</td><td>600</td></tr>\n",
    "    <tr><td>5</td><td>300</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<table style=\"display: inline-block;\">\n",
    "  <caption><strong>Bigrams with History \"wine\"</strong></caption>\n",
    "  <thead>\n",
    "    <tr><th>Count (N(w,h))</th><th>Bigram</th></tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>1</td><td>wine drinker</td></tr>\n",
    "    <tr><td>2</td><td>wine lover</td></tr>\n",
    "    <tr><td>3</td><td>wine glass</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "a. What are the discounted counts under Good–Turing discounting for the three given bigrams?\n",
    "\n",
    "b. The amounts from discounting counts are given to a back-off unigram model. Using such a back-off model, what are the\n",
    "probabilities for the following bigrams?\\\n",
    "(i) $p(drinker|wine)$\\\n",
    "(ii) $p(glass|wine)$\\\n",
    "(iii) $p(mug|wine)$\\\n",
    "Note: $p(mug) = 0.015$, $p(drinker)=0.015$, $p(glass)=0.01$ . State any assumptions that you make.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ad7a7",
   "metadata": {},
   "source": [
    "# Answers - Exercise 2\n",
    "\n",
    "(a) Good-Turing discounted count has the formula:\n",
    "$$N*=\\frac{(r+1)*n_{r+1}}{n_r}$$\n",
    "where, \n",
    "* r: observed count of the bigram (e.g., 1, 2, 3)\n",
    "* $n_r$ : number of bigrams with count \n",
    "* $n_{r+1}$ : number of bigrams with count r+1\n",
    "* N∗ : discounted count\n",
    "\n",
    "For 1. wine drinker (r=1)\n",
    "$$N*=\\frac{(1+1)*n_2}{n_1} =\\frac{(1+1)*1600}{5000} = 0.64$$\n",
    "For 2. wine lover (r=2)\n",
    "$$N*=\\frac{(2+1)*n_3}{n_2} =\\frac{(2+1)*1000}{1600} = 1.89$$\n",
    "For 3. wine glass (r=3)\n",
    "$$N*=\\frac{(3+1)*n_4}{n_4} =\\frac{(3+1)*600}{1000} = 2.40$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "(b) Lets assume counts are normalized over the total discounted counts for \"wine\". We can then normalize each discounted count by this total to get actual bigram probabilities.<br/>\n",
    "So the total discounted count for history \"wine\" is:\n",
    "$$0.64+1.89+2.40=4.93$$\n",
    "Now we can normalize (at least for those bigrams that are seen)<br/>\n",
    "(i) p( drinker | wine ) = 0.64/4.93 = 0.129<br/>\n",
    "(ii) p( glass | wine ) = 2.40/4.93 = 0.487<br/>\n",
    "\n",
    "(iii) Because of our assumption, we can say that ALL MASS has been assigned to seen bigrams (4.93/4.93) = 1. <br/>\n",
    "However, Good-Turing discounting assumes that some mass should be left for unseen events. But since we are using discounted counts, not raw counts, we should normalize over the original total count, not the discounted one. Let's assume the original total count (before discounting) is:\n",
    "$$1(drinker)+2(lover)+3(glass) = 6$$\n",
    "Then:\n",
    "$$Total discounted mass=0.64+1.875+2.4=4.915$$\n",
    "$$Leftover mass=6−4.915=1.085$$\n",
    "\n",
    "So, leftover probability mass:\n",
    "\n",
    "$$\\alpha = \\frac{1.085}{6} = 0.1808$$\n",
    "This leftover mass is distributed among UNSEEN BIGRAMS using smoothed unigram probabilities. <br/>\n",
    "(iii) p( mug | wine ) <br/>\n",
    "To simplify, assume mug is the only unseen continuation under wine (or normalize later if more are known).\n",
    "$$p(mug|wine)=\\alpha * \\frac{p(mug)}{Z}$$\n",
    "\n",
    "Where Z is the total unigram mass of all unseen continuations.\n",
    "Assuming only mug is unseen (as stated):\n",
    "$$Z=p(mug)=0.015$$\n",
    "$$p(mug|wine)=0.1808 * \\frac{0.015}{0.015}=0.1808$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ddacb",
   "metadata": {},
   "source": [
    "## Exercise 3: Cross-Validation (5 points)\n",
    "\n",
    "\n",
    "Imagine you are a linguist in the remote future who just rediscovered a book called \"Bible\". To your disappointment the book is obviously incomplete; all the pages between Genesis and the Apocalypse are torn out, maybe by some late Christian cult. Since you don't know the language of the book you want to build a first language model that you can use if you find any of the lost parts. You digitize the book with your state-of-the art portable digitizer, and then load it into one of your Python notebooks.\n",
    "\n",
    "### 3.1 Baseline\n",
    "\n",
    "* The two corpora are in the text files `genesis.txt` and `apocalypsis.txt`. Load them into the notebook, preprocess them by removing all non-alphabetical characters, and then concatenate them into a single corpus. Split the corpus into a train and a test set, with the test set comprising the _last_ 20% of the corpus. The functions for `preprocess` and `train_test_split_data` should be implemented in `exercise_3.py` (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import exercise_3\n",
    "exercise_3 = reload(exercise_3)\n",
    "\n",
    "with open('data/genesis.txt', 'r', encoding='utf-8') as file:\n",
    "    genesis_lines = file.readlines()\n",
    "\n",
    "with open('data/apocalypsis.txt', 'r', encoding='utf-8') as file:\n",
    "    apocalypsis_lines = file.readlines()\n",
    "\n",
    "genesis_text = genesis_lines\n",
    "apocalypsis_text = apocalypsis_lines\n",
    "\n",
    "# preprocess\n",
    "genesis_preprocessed = exercise_3.preprocess(genesis_text)\n",
    "apocalypsis_preprocessd = exercise_3.preprocess(apocalypsis_text)\n",
    "\n",
    "# concatenate\n",
    "corpus = genesis_preprocessed + apocalypsis_preprocessd\n",
    "#print(corpus)\n",
    "\n",
    "# train, test split\n",
    "train, test = exercise_3.train_test_split_data(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb4917",
   "metadata": {},
   "source": [
    "* Using the language model class given, we will estimate a trigram language model on the train set and report perplexity on the test set. First, implement the `perplexity` function in `language_model.py` then calculate it using $\\alpha=1$. Does this represent an unbiased estimate of the model's capacity? (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09333483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509.9120526977413\n"
     ]
    }
   ],
   "source": [
    "# TODO: trigram LM\n",
    "from importlib import reload\n",
    "#from language_model import NGramLM\n",
    "import language_model\n",
    "language_model = reload(language_model)\n",
    "\n",
    "\n",
    "N = 3 #trigram\n",
    "alpha = 1.0\n",
    "\n",
    "LM = language_model.NGramLM(train, N, alpha)\n",
    "\n",
    "perplexity = LM.perplexity(test)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e3a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b76c2a1",
   "metadata": {},
   "source": [
    "### 3.2 Average Perplexity\n",
    "\n",
    "* Since you want to get an unbiased estimate of your model's capacity, you decide to apply k-fold cross-validation on your corpus. To do this, implement the function `k_validation_folds` in `exercise_3.py`. Use it to split your corpus into $k=5$ cross-validation folds, and make sure that the folds are of the same size. (1 point)\n",
    "\n",
    "* Now, estimate a trigram language model on each of the CV folds. Use the `NGramLM` class, and average over all perplexity scores. Does the average score differ from the one obtained in 3.2, and why? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10f688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 5-fold CV perplexity: 1223.9571967405884\n"
     ]
    }
   ],
   "source": [
    "exercise_3 = reload(exercise_3)\n",
    "import language_model\n",
    "language_model = reload(language_model)\n",
    "\n",
    "\n",
    "\n",
    "# 10-fold cross-validation\n",
    "cv_folds = exercise_3.k_validation_folds(corpus, k_folds=5) #the comment says 10 but this was placed as 5\n",
    "#print(cv_folds)\n",
    "\n",
    "pps = []\n",
    "\n",
    "# TODO: estimate 10 trigram LMs!\n",
    "k_folds = 5 #makign this into a variable because the comments and questions are contradicting :/\n",
    "for i in range(k_folds):\n",
    "    #train and test split\n",
    "    test_fold = cv_folds[i]\n",
    "    train_folds = cv_folds[:i] + cv_folds[i+1:]\n",
    "    train_corpus = [token for fold in train_folds for token in fold]\n",
    "    \n",
    "    #train trigram model again\n",
    "    model = language_model.NGramLM(train_corpus, N=3, alpha=0.1)  # example smoothing alpha\n",
    "    \n",
    "    #calculate perplexity on test fold\n",
    "    pp = model.perplexity(test_fold)\n",
    "    pps.append(pp)\n",
    "\n",
    "# Average perplexity over folds\n",
    "average_pp = sum(pps) / len(pps)\n",
    "print(f\"Average 5-fold CV perplexity: {average_pp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d6a63",
   "metadata": {},
   "source": [
    "Each fold uses a different test set and train set, so the model is trained on slightly different data each time, which affects the perplexity in a good way, and we can see the perplexity decreasing as well. Averaging over folds reduces variance and gives a more unbiased and robust estimate than a single train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f5ba0",
   "metadata": {},
   "source": [
    "### 3.3 Hyperparameter Tuning\n",
    "\n",
    "* As you don't know anything about the language the book is written in, you have to find the best hyperparemter $\\alpha$ for your model by a brute-force search. Since you know from 3.2 that your data is not balanced, you decide to use only the averaged perplexity score (derived from $k=10$ CV folds) for this. Do so by completing the loop in the code cell below. Then, plot the obtained perplexity scores vs. $\\alpha$. Implement the function `plot_pp_vs_alpha` in `exercise_3.py` for this. (1 point)\n",
    "\n",
    "**Hint:** This could take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c633b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamter tuning, CV for trigram\n",
    "alphas = [x*0.01 for x in range(1,101)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d91e0",
   "metadata": {},
   "source": [
    "* Repeat the tuning process for unigram and bigram language models. Does your estimate of $\\alpha$ differ? Why? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a34143d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hyperparamter tuning, unigram + bigram\n",
    "language_model = reload(language_model)\n",
    "exercise_3 = reload(exercise_3)\n",
    "\n",
    "\n",
    "pps = []\n",
    "\n",
    "for alpha in alphas:\n",
    "  # TODO: estimate LMs!\n",
    "  fold_pps = []\n",
    "  for i in range(k_folds):\n",
    "\n",
    "\n",
    "\n",
    "    test_fold = cv_folds[i]\n",
    "    train_folds = cv_folds[:i] + cv_folds[i+1:]\n",
    "    train_corpus = [token for fold in train_folds for token in fold]\n",
    "\n",
    "    #train trigram LM with smoothing alpha\n",
    "    model = language_model.NGramLM(train_corpus, N=3, alpha=alpha)\n",
    "    pp = model.perplexity(test_fold)\n",
    "    fold_pps.append(pp)\n",
    "  \n",
    "  avg_pp = sum(fold_pps) / k_folds\n",
    "  pps.append(avg_pp)\n",
    "\n",
    "# TODO: plot!\n",
    "\n",
    "exercise_3.plot_pp_vs_alpha(pps, alphas, N=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cabc2",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Bonus Question (2 points)\n",
    "\n",
    "Read the Research Paper, [Tokenization and the Noiseless Channel](https://arxiv.org/pdf/2306.16842) . The author proposes a new way to evaluate tokenization techniques in NLP using information theory. Specifically, the author introduces the concept of \"channel efficiency\" which measures the information in a model, received from the tokenizer, via Rényi entropy. The authors find that a Rényi entropy with α = 2.5 correlates strongly (0.78) with BLEU scores in machine translation tasks, outperforming traditional metrics like compressed length.\n",
    "\n",
    "\n",
    "Discuss the reasoning behind using Rényi entropy as an evaluator of tokenization efficiency, over Shannon entropy. Furthermore, discuss the implications of this finding for the design of tokenizers in NLP pipelines, particularly in tasks like machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91aeec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The paper \"Tokenization and the Noiseless Channel\" by Zouhar et al. introduces an innovative approach to evaluating tokenization techniques in natural language processing (NLP) using information theory. Specifically, the authors propose the concept of \"channel efficiency,\" measured via Rényi entropy, to assess how effectively a tokenizer conveys information to a model. Their findings indicate that a Rényi entropy with α = 2.5 correlates strongly (0.78) with BLEU scores in machine translation tasks, outperforming traditional metrics like compressed length.\n",
    "\n",
    "Why Use Rényi Entropy Over Shannon Entropy?\n",
    "\n",
    "Shannon entropy (α = 1) measures the average information content in a distribution, emphasizing the most probable events. While useful, it can assign extremely long codes to low-frequency tokens and very short codes to high-frequency tokens, potentially leading to inefficiencies in token representation.\n",
    "\n",
    "Rényi entropy generalizes Shannon entropy by introducing an order parameter α, allowing for different sensitivities to the distribution's tail. At α = 2.5, Rényi entropy penalizes distributions with either very high or very low-frequency tokens. This balance discourages tokenizations that overemphasize either end of the frequency spectrum, promoting a more uniform distribution that can be more effective for model learning.\n",
    "\n",
    "Implications for Tokenizer Design in NLP Pipelines\n",
    "\n",
    "The strong correlation between Rényi entropy (α = 2.5) and BLEU scores suggests that tokenizers producing balanced token frequency distributions can enhance model performance in machine translation tasks. This insight can guide the development of tokenization strategies that avoid over-representing common tokens or under-representing rare ones, leading to more efficient and effective NLP models .\n",
    "\n",
    "However, it's important to note that while Rényi entropy provides valuable insights, it is not a definitive predictor of model performance. Subsequent studies have identified scenarios where increasing Rényi efficiency does not correspond to improved downstream performance. For instance, modifications like Random-Drop BPE and Duplication BPE can artificially inflate Rényi efficiency while degrading model quality.\n",
    "\n",
    "Therefore, while Rényi entropy is a useful tool for evaluating tokenization schemes, it should be used in conjunction with other metrics and domain-specific considerations to design effective tokenizers for NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030c33b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
